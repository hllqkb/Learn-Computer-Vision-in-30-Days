{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e7dbd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
      "  Cloning https://github.com/facebookresearch/segment-anything.git to c:\\users\\hllqkb\\appdata\\local\\temp\\pip-req-build-94pf9p9z\n",
      "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: segment_anything\n",
      "  Building wheel for segment_anything (setup.py): started\n",
      "  Building wheel for segment_anything (setup.py): finished with status 'done'\n",
      "  Created wheel for segment_anything: filename=segment_anything-1.0-py3-none-any.whl size=36875 sha256=a153e7033e381fb5d23835e4809a29f11a1a6a8d0c9d1320dd961ab42bae2671\n",
      "  Stored in directory: C:\\Users\\hllqkb\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-xbt9md5k\\wheels\\15\\d7\\bd\\05f5f23b7dcbe70cbc6783b06f12143b0cf1a5da5c7b52dcc5\n",
      "Successfully built segment_anything\n",
      "Installing collected packages: segment_anything\n",
      "Successfully installed segment_anything-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git 'C:\\Users\\hllqkb\\AppData\\Local\\Temp\\pip-req-build-94pf9p9z'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\hllqkb\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hllqkb\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\hllqkb\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy in c:\\users\\hllqkb\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\hllqkb\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hllqkb\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\hllqkb\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hllqkb\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hllqkb\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hllqkb\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hllqkb\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hllqkb\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hllqkb\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
    "!pip install torch torchvision opencv-python numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebc2a653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth to ./sam_vit_b_01ec64.pth ...\n",
      "Download complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hllqkb\\anaconda3\\envs\\pytorch-gpu\\Lib\\site-packages\\segment_anything\\build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    }
   ],
   "source": [
    "# Create SAM predictor\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from segment_anything import SamPredictor,sam_model_registry\n",
    "import urllib.request\n",
    "\n",
    "def wget(url, filename=None):\n",
    "    \"\"\"\n",
    "    下载文件到当前文件夹，类似wget命令。\n",
    "    url: 文件下载链接\n",
    "    filename: 保存的文件名（可选），不填则自动取url最后一段\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        filename = url.split('/')[-1]\n",
    "    print(f\"Downloading {url} to {filename} ...\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "# Download the model if not exists\n",
    "model_path = './sam_vit_b_01ec64.pth'\n",
    "if not os.path.exists(model_path):\n",
    "    wget(\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\", model_path)\n",
    "# Load the predictor\n",
    "sam=sam_model_registry[\"vit_b\"](checkpoint=model_path)\n",
    "predictor=SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be39a054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://utils-computervisiondeveloper.s3.amazonaws.com/media/public/test.jpg to ./test.jpg ...\n",
      "Download complete.\n"
     ]
    }
   ],
   "source": [
    "# load image and select x,y coordinates\n",
    "image_path = './test.jpg'\n",
    "if not os.path.exists(image_path):\n",
    "  # !wget https://utils-computervisiondeveloper.s3.amazonaws.com/media/public/test.jpg\n",
    "  wget(\"https://utils-computervisiondeveloper.s3.amazonaws.com/media/public/test.jpg\", image_path)\n",
    "\n",
    "x = 528\n",
    "y = 606\n",
    "\n",
    "image = cv2.imread(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e40d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sam predictor on (image, x, y) to get location of subject\n",
    "predictor.set_image(image)\n",
    "masks,scores,logits = predictor.predict(\n",
    "    point_coords=np.array([[x, y]]),\n",
    "    point_labels=np.array([1]),\n",
    "    multimask_output=True\n",
    ")\n",
    "C,H,W = masks.shape\n",
    "result_mask = np.zeros((H, W), dtype=bool)\n",
    "\n",
    "for j in range(C):\n",
    "  result_mask |= masks[j, :, :]\n",
    "\n",
    "result_mask = result_mask.astype(np.uint8)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc0e9f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove background\n",
    "alpha_channel = np.ones(image.shape[:2], dtype=np.uint8) * 255\n",
    "alpha_channel[result_mask == 0] = 0\n",
    "bg_removed = cv2.cvtColor(image, cv2.COLOR_BGR2BGRA)\n",
    "bg_removed[:, :, 3] = alpha_channel\n",
    "cv2.imwrite('bg_removed.png', bg_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc8ca17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './sam_vit_b_01ec64.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(model_path):\n\u001b[32m     13\u001b[39m   get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mwget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m sam = \u001b[43msam_model_registry\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvit_b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m predictor = SamPredictor(sam)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mremove_background\u001b[39m(image_base64_encoding, x, y):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hllqkb\\anaconda3\\envs\\pytorch-gpu\\Lib\\site-packages\\segment_anything\\build_sam.py:38\u001b[39m, in \u001b[36mbuild_sam_vit_b\u001b[39m\u001b[34m(checkpoint)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_sam_vit_b\u001b[39m(checkpoint=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_build_sam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_embed_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m768\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_num_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_global_attn_indexes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m11\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hllqkb\\anaconda3\\envs\\pytorch-gpu\\Lib\\site-packages\\segment_anything\\build_sam.py:104\u001b[39m, in \u001b[36m_build_sam\u001b[39m\u001b[34m(encoder_embed_dim, encoder_depth, encoder_num_heads, encoder_global_attn_indexes, checkpoint)\u001b[39m\n\u001b[32m    102\u001b[39m sam.eval()\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    105\u001b[39m         state_dict = torch.load(f)\n\u001b[32m    106\u001b[39m     sam.load_state_dict(state_dict)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './sam_vit_b_01ec64.pth'"
     ]
    }
   ],
   "source": [
    "# wrap it up as a function \n",
    "# wrap it up as a function\n",
    "import base64\n",
    "import os\n",
    "\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model_path = './sam_vit_b_01ec64.pth'\n",
    "if not os.path.exists(model_path):\n",
    "  !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
    "\n",
    "sam = sam_model_registry[\"vit_b\"](checkpoint=model_path)\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "\n",
    "def remove_background(image_base64_encoding, x, y):\n",
    "\n",
    "  image_bytes = base64.b64decode(image_base64_encoding)\n",
    "\n",
    "  image = cv2.imdecode(np.frombuffer(image_bytes, dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "\n",
    "  predictor.set_image(image)\n",
    "\n",
    "  masks, scores, logits = predictor.predict(\n",
    "                                    point_coords=np.asarray([[x, y]]),\n",
    "                                    point_labels=np.asarray([1]),\n",
    "                                    multimask_output=True\n",
    "                                )\n",
    "\n",
    "  C, H, W = masks.shape\n",
    "\n",
    "  result_mask = np.zeros((H, W), dtype=bool)\n",
    "\n",
    "  for j in range(C):\n",
    "    result_mask |= masks[j, :, :]\n",
    "\n",
    "  result_mask = result_mask.astype(np.uint8)\n",
    "\n",
    "  alpha_channel = np.ones(result_mask.shape, dtype=result_mask.dtype) * 255\n",
    "\n",
    "  alpha_channel[result_mask == 0] = 0\n",
    "\n",
    "  result_image = cv2.merge((image, alpha_channel))\n",
    "\n",
    "  _, result_image_bytes = cv2.imencode('.png', result_image)\n",
    "\n",
    "  result_image_bytes = result_image_bytes.tobytes()\n",
    "\n",
    "  result_image_bytes_encoded_base64 = base64.b64encode(result_image_bytes).decode('utf-8')\n",
    "\n",
    "  return result_image_bytes_encoded_base64\n",
    "     \n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "image_path = './test.jpg'\n",
    "if not os.path.exists(image_path):\n",
    "  !wget https://utils-computervisiondeveloper.s3.amazonaws.com/media/public/test.jpg\n",
    "\n",
    "x = 528\n",
    "y = 606\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "_, image_bytes = cv2.imencode('.png', image)\n",
    "\n",
    "image_bytes = image_bytes.tobytes()\n",
    "\n",
    "image_bytes_encoded_base64 = base64.b64encode(image_bytes).decode('utf-8')\n",
    "\n",
    "result_image = remove_background(image_bytes_encoded_base64, x, y)\n",
    "\n",
    "result_image_bytes = base64.b64decode(result_image)\n",
    "\n",
    "result_image = cv2.imdecode(np.frombuffer(result_image_bytes, dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGRA2RGBA))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3776b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
